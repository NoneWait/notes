# 11月主要论文整理

最近刚写完毕设和实习复现论文的工作，
打算浏览下近期的预训练语言模型的论文
（毕设里简单介绍了下这个的思想，并且以后工作
了可能会做相关的工作，所以在这学习下）。
我们可以以[Glue](https://gluebenchmark.com/leaderboard/)
榜单来check下最近的预训练语言模型的论文。
据目前所知的情况来看，可以分为几个部分：

- 修改bert的模型：参数量|本身框架
- 修改预训练目标
- 添加一些先验知识

## 2019-11-18

### 修改bert模型

1. [xlnet](https://arxiv.org/abs/1906.08237)
2. [albert](https://arxiv.org/pdf/1909.11942.pdf)

出发点：目前的预训练模型参数量太大，降低模型参数量
贡献：设计了两种减少参数量的技术，设计了一个自监督损失来建模句内
共现，有利于多句输入的下游任务
code: https://github.com/google-research/google-research/tree/master/albert

摘要：

- 作者发现：训练步不够时bert模型性能不佳，增倍bert-large的hidden_size性能也不佳(why？)
- 要设计一个模型：address the memory limitation problem， the communication overhead
- and model degradation，也就是说既要降低参数量又要保证模型不退化，所以设计了模型ALBERT(A Lite BERT)

模型：

- factorized embedding parameterization：把词表的embedding matrix分解为两个子矩阵
- cross-layer parameter sharing：随着网络加深保留参数->意思就是底层的参数给上层共享，
    也就是多层共享一个参数，而不是每层都是一个新参数(所有层只有一组参数？)
- SOP: sentence-order prediction:改进NSP

details:

- 对于wordpiece embedding学习的是context-independent表示（和词向量一样，与上下文无关），
而hidden-layer embedding则学的是context-dependent表示。所以，因为要更需要学到的
是上下文相关的表示，这也是为什么H>>E的原因，原有的模型的词向量矩阵是$V \times E$。
所以该方法就是先将词表映射到低维向量，再从低维升到hidden_size，则参数量则从
$O(V\times H)$到$O(V \times E+ E \times H)$，
当H>>E时参数量会大幅度减少。那么我们的E要设多大呢？在
ALBERTZ中是设为128。

- 跨层的参数共享：可以共享FFN，也可以共享attention parameters，ALBERT的选择是共享所有参数

- Inter-sentence coherence loss:使用SOPloss来实现，
正例和NSP中一样生成，而负例则用同样相邻的两个片段但是
它们的顺序调换。

实验结果：还是很牛逼的，参数量降低了10倍和base版的bert效果差不多，通过增大H的大小，能够达到更好的效果。在Squad上，(xlarge(ALBERT)60M->bert_large(334M))

### 预训练任务 or 增强训

1. [Roberta]()
2. [SpanBERT]()
3. [T5]()

### 先验知识

1. [ernie2.0]()
