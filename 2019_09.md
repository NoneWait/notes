# 2019年9月

## 2019-9-14
1. [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)

这篇提出了Batch Normalization，并介绍了BN几个应用场景。

2. [KagNet: Knowledge-Aware Graph Networks
for Commonsense Reasoning](https://arxiv.org/pdf/1909.02151.pdf)

EMNLP2019的文章，主要思路是在Commonsense QA的QA pair对抽取相关的子图，利用GCNs+LSTMs生成子图的表示来做Knowledge Enhance。

构子图的方法中，利用一条路径中三元组的得分乘积来过滤一些置信度低的路径，或者说是剪枝。(weak)。

3. [Contextual Word Representations: A Contextual Introduction](https://arxiv.org/pdf/1902.06006.pdf)

从离散词的表示到词向量再到蕴含上下文的分布式词向量表示，最后到Contextual Word Vectors的一个脉络的总结和展望。

4. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

transform框架的由来，主要是针对几个问题重读了下这个论文，（1）在self-attention的时候为什么要出于维度的平方？论文的解释是点乘attention会使得一些数非常大(可是是因为d_k的原因，d_k越大求和项会变得很多)，而数值变大会导致softmax落入梯度小的region，导致梯度消失，所以用这个分母来进行缩放。(2)为什么要用多头注意力，可以针对不同位置联合获得不同的表示子空间。之前理解错了，以为是为了增加参数量，实际多头是把512/n_head，参数量是不变的。