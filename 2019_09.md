# 2019年9月

## 2019-9-14
1. [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)

这篇提出了Batch Normalization，并介绍了BN几个应用场景。

2. [KagNet: Knowledge-Aware Graph Networks
for Commonsense Reasoning](https://arxiv.org/pdf/1909.02151.pdf)

EMNLP2019的文章，主要思路是在Commonsense QA的QA pair对抽取相关的子图，利用GCNs+LSTMs生成子图的表示来做Knowledge Enhance。

构子图的方法中，利用一条路径中三元组的得分乘积来过滤一些置信度低的路径，或者说是剪枝。(weak)。

3. [Contextual Word Representations: A Contextual Introduction](https://arxiv.org/pdf/1902.06006.pdf)

从离散词的表示到词向量再到蕴含上下文的分布式词向量表示，最后到Contextual Word Vectors的一个脉络的总结和展望。

4. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

transform框架的由来，主要是针对几个问题重读了下这个论文，（1）在self-attention的时候为什么要出于维度的平方？论文的解释是点乘attention会使得一些数非常大(可是是因为d_k的原因，d_k越大求和项会变得很多)，而数值变大会导致softmax落入梯度小的region，导致梯度消失，所以用这个分母来进行缩放。(2)为什么要用多头注意力，可以针对不同位置联合获得不同的表示子空间。之前理解错了，以为是为了增加参数量，实际多头是把512/n_head，参数量是不变的。

## 2019-9-15

5. [Interactive Language Learning by Question Answering](https://arxiv.org/pdf/1908.10909.pdf)

介绍了一个任务：Question Answering with Interactive Text(QAit)

目前的MRC数据集有三个主要的问题：
(1)鼓励模型通过shallow phrase和word matching来进行问题和知识之间的匹配。所以通过改写的和排列就是一个比较有效的回答答案的策略。一个简单的IR方法在Squad上可以取得比较高句子级的性能。
(2)这些问题的答案在source（这里指的是para）里往往是静态而且信息充分的。所以会导致回答这些问题不是像人类那样会和世界知识做一个交互。也就是information-seeking的过程。
(3) 现有的MRC主要集中在陈述性知识(declarative knowledge)，就是这种知识的fact和event可以在一个非常短的文本中陈述出来。所以很容易通过匹配模型抽取出正确的答案。比如说给定EMNLP文本，会议的日期可以通过匹配date mention来抽取。所以本文关注(procedural knowledge)，即是蕴含了一系列的动作。比如上述操作，先搜索EMNLP，点进链接，然后找date mention，这是一系列要完成的动作。

则这个任务实际上是给定一个问题，和一个环境，agent需要做出一系列动作决策回答答案。

[数据和baseline](https://github.com/xingdi-eric-yuan/qait_public)

6. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555.pdf)

这篇主要是比较了下不同的Gated RNN，主要是GRU和LSTM。

为了解决梯度衰减，主要是延伸了两种研究路线：
(1)设计更好的学习算法，比如a simple stochastic gradient descent，比如梯度裁剪，或者利用二阶梯度的方法
(2)设计一个更复杂的激活函数，利用门单元来做简单的非线性点乘来跟随一个彷射。比如LSTM或者GRU。